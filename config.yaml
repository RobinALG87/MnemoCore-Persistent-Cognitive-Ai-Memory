# HAIM Configuration — Phase 4.5
# All hardcoded constants are centralized here.

haim:
  version: "5.0"
  dimensionality: 16384  # 2^14, must be multiple of 64

  # Vector encoding
  encoding:
    mode: "binary"  # "binary" (Phase 3.0+) or "float" (legacy)
    token_method: "bundle"  # "bundle" (XOR+permute) or "hash"

  # Memory tier thresholds
  tiers:
    hot:
      max_memories: 3000
      ltp_threshold_min: 0.7
      eviction_policy: "lru"

    warm:
      max_memories: 100000
      ltp_threshold_min: 0.3
      consolidation_interval_hours: 1
      storage_backend: "mmap"  # "mmap" (Phase 3.0) or "qdrant" (Phase 3.5)

    cold:
      storage_backend: "filesystem"  # "filesystem" or "s3"
      compression: "gzip"
      archive_threshold_days: 30

  # LTP (Long-Term Potentiation) decay parameters
  ltp:
    initial_importance: 0.5
    decay_lambda: 0.01  # Exponential decay rate
    permanence_threshold: 0.95  # Above this, memory is immune to decay
    half_life_days: 30.0  # For synaptic connections

  # Hysteresis (prevent boundary thrashing between tiers)
  hysteresis:
    promote_delta: 0.15  # LTP must exceed threshold by this much to promote
    demote_delta: 0.10   # LTP must fall below threshold by this much to demote

  # Redis (Phase 3.5)
  redis:
    url: "redis://localhost:6379/0"
    stream_key: "haim:subconscious"
    max_connections: 10
    socket_timeout: 5

  # Qdrant (Phase 3.5)
  qdrant:
    url: "http://localhost:6333"
    collection_hot: "haim_hot"
    collection_warm: "haim_warm"
    binary_quantization: true
    always_ram: true
    hnsw_m: 16
    hnsw_ef_construct: 100

  # GPU (Phase 3.5)
  gpu:
    enabled: false
    device: "cuda:0"
    batch_size: 1000
    fallback_to_cpu: true

  # Observability (Phase 3.5)
  observability:
    metrics_port: 9090
    log_level: "INFO"
    structured_logging: true

  # Persistence paths
  paths:
    data_dir: "./data"
    memory_file: "./data/memory.jsonl"
    codebook_file: "./data/codebook.json"
    concepts_file: "./data/concepts.json"
    synapses_file: "./data/synapses.json"
    warm_mmap_dir: "./data/warm_tier"
    cold_archive_dir: "./data/cold_archive"

  # Security (Phase 3.5.1)
  security:
    # api_key: "mnemocore-beta-key"  # <--- REMOVED: Must be set via HAIM_API_KEY env var or here explicitly

  # MCP (Model Context Protocol) bridge
  mcp:
    enabled: true
    transport: "stdio"  # "stdio" recommended for local MCP clients
    host: "127.0.0.1"
    port: 8110
    api_base_url: "http://localhost:8100"
    timeout_seconds: 15
    allow_tools:
      - "memory_store"
      - "memory_query"
      - "memory_get"
      - "memory_delete"
      - "memory_stats"
      - "memory_health"

  # Dream Loop (Subconscious background processing)
  dream_loop:
    enabled: true
    frequency_seconds: 60  # Seconds between dream cycles
    batch_size: 10  # Number of memories to process per cycle
    max_iterations: 0  # Maximum iterations (0 = unlimited)
    subconscious_queue_maxlen: 10000  # Max queued IDs (null/0 = unlimited)
    ollama_url: "http://localhost:11434/api/generate"
    model: "gemma3:1b"

  # Phase 4.0+: Semantic Consolidation
  consolidation:
    enabled: true
    interval_seconds: 3600  # 1 hour between consolidation cycles
    similarity_threshold: 0.85  # Hamming similarity threshold (0.85 = 15% distance)
    min_cluster_size: 2  # Minimum cluster size for merging
    hot_tier_enabled: true  # Consolidate HOT tier
    warm_tier_enabled: true  # Consolidate WARM tier

  # Phase 4.1: XOR-based Project Isolation
  attention_masking:
    enabled: true  # Enable/disable project-based memory isolation

  # Phase 4.6: Hybrid Search (Dense + Sparse)
  search:
    mode: "hybrid"  # "dense" | "sparse" | "hybrid"
    hybrid_alpha: 0.7  # Weight for dense search in hybrid mode (0.0-1.0)
    rrf_k: 60  # RRF constant for rank fusion (higher = more ranking focused)
    sparse_model: "bm25"  # "bm25" or SPLADE model path
    enable_query_expansion: true  # Enable query expansion for better recall
    min_dense_score: 0.0  # Minimum dense score threshold
    min_sparse_score: 0.0  # Minimum sparse score threshold

  # =========================================================================
  # Subconscious AI - BETA FEATURE
  # =========================================================================
  # This is a BETA feature that enables autonomous background AI processing
  # for memory management, dream synthesis, and micro-self-improvement.
  #
  # WARNING: This feature is experimental and may change without notice.
  # Must be explicitly enabled by setting 'enabled: true'.
  # All safety defaults are conservative - review before enabling in production.
  # =========================================================================
  # =========================================================================
  # Subconscious AI — Phase 4.4+ (Autonomous Background Intelligence)
  # =========================================================================
  #
  # A small language model that runs continuously in the background,
  # performing three cognitive operations in rotation:
  #
  #   1. MEMORY SORTING    — categorizes and tags unclassified memories
  #   2. ENHANCED DREAMING — synthesizes insights by connecting related memories
  #   3. MICRO SELF-IMPROVEMENT — detects patterns and proposes structural changes
  #
  # The worker respects CPU thresholds and rate limits so it never competes
  # with foreground workloads. All decisions are written to an audit trail.
  #
  # SUPPORTED PROVIDERS
  # -------------------
  #   ollama        — Local models via Ollama (recommended for home use)
  #   lm_studio     — Local models via LM Studio (OpenAI-compatible endpoint)
  #   openai_api    — OpenAI GPT-4o, GPT-4o-mini, etc.
  #   anthropic_api — Anthropic Claude 3 Haiku, Sonnet, Opus
  #
  # QUICK START (Ollama on the same machine)
  # ----------------------------------------
  #   1. Install Ollama:  https://ollama.ai
  #   2. Pull a model:    ollama pull phi3.5
  #   3. Set enabled: true below
  #   4. Restart MnemoCore
  #
  # See docs/SUBCONSCIOUS_AI.md for full setup guide and model recommendations.
  # =========================================================================
  subconscious_ai:

    # ── Master switch ──────────────────────────────────────────────────────
    enabled: true          # Set to false to completely disable
    beta_mode: true        # Extra safety logging (recommended to keep on)

    # ── Model provider ────────────────────────────────────────────────────
    # Choose ONE provider block. Comment out the others.
    #
    # ┌──────────────────────────────────────────────────────────────────┐
    # │ OPTION A — Ollama (local, recommended for home/dev)             │
    # │   ollama pull phi3.5          ~2.5 GB VRAM, fast on CPU         │
    # │   ollama pull llama3.2:3b     ~2 GB, excellent quality          │
    # │   ollama pull mistral:7b      ~4 GB, best reasoning             │
    # │   ollama pull gemma3:4b       ~3 GB, strong coding              │
    # │   ollama pull qwen2.5:3b      ~2 GB, great multilingual         │
    # └──────────────────────────────────────────────────────────────────┘
    model_provider: "ollama"
    model_name: "phi3.5:latest"       # Change to any model you have pulled
    model_url: "http://localhost:11434"
    #
    # NOTE for Docker users: MnemoCore can't reach "localhost" on the host.
    # Use host.docker.internal instead, or set the env var:
    #   SUBCONSCIOUS_AI_MODEL_URL=http://host.docker.internal:11434
    #
    # ┌──────────────────────────────────────────────────────────────────┐
    # │ OPTION B — LM Studio (local, OpenAI-compatible server)          │
    # │   Open LM Studio → Local Server tab → Start Server              │
    # └──────────────────────────────────────────────────────────────────┘
    # model_provider: "lm_studio"
    # model_name: "phi-3.5-mini-instruct"  # Must match what is loaded
    # model_url: "http://localhost:1234"   # LM Studio default port
    #
    # ┌──────────────────────────────────────────────────────────────────┐
    # │ OPTION C — OpenAI API                                           │
    # │   Set env var: SUBCONSCIOUS_AI_API_KEY=sk-...                   │
    # └──────────────────────────────────────────────────────────────────┘
    # model_provider: "openai_api"
    # model_name: "gpt-4o-mini"            # Cost-effective, fast
    # model_url: "https://api.openai.com"
    # api_key: null                        # Use SUBCONSCIOUS_AI_API_KEY env var
    #
    # ┌──────────────────────────────────────────────────────────────────┐
    # │ OPTION D — Anthropic Claude                                     │
    # │   Set env var: SUBCONSCIOUS_AI_API_KEY=sk-ant-...               │
    # └──────────────────────────────────────────────────────────────────┘
    # model_provider: "anthropic_api"
    # model_name: "claude-3-haiku-20240307"
    # model_url: "https://api.anthropic.com"
    # api_key: null                        # Use SUBCONSCIOUS_AI_API_KEY env var

    # ── Pulse timing ──────────────────────────────────────────────────
    pulse_interval_seconds: 120       # Seconds between pulses (2 min default)
    pulse_backoff_enabled: true       # Increase interval on repeated LLM errors
    pulse_backoff_max_seconds: 600    # Maximum backoff cap: 10 minutes

    # ── Resource protection ───────────────────────────────────────────
    # Pulse is skipped entirely if the machine is under load.
    # Requires psutil (included in requirements.txt).
    max_cpu_percent: 30.0             # Skip if CPU > this percentage
    cycle_timeout_seconds: 120        # Max seconds to wait for a model response
    rate_limit_per_hour: 50           # Max total LLM calls per hour

    # ── Operations ────────────────────────────────────────────────────
    memory_sorting_enabled: true      # Categorize and tag unclassified memories
    enhanced_dreaming_enabled: true   # Generate insights from memory clusters
    micro_self_improvement_enabled: false  # Pattern analysis (Phase 0, dry-run)

    # ── Safety ────────────────────────────────────────────────────────
    # dry_run: true  → generates suggestions but never writes to memory.
    # dry_run: false → suggestions ARE applied (tags written, synths stored).
    # Start with dry_run: true to observe proposals before committing.
    dry_run: false
    log_all_decisions: true
    audit_trail_path: "./data/subconscious_audit.jsonl"
    max_memories_per_cycle: 10

  # =========================================================================
  # Dream Scheduler - Phase 6.0: Offline Consolidation
  # =========================================================================
  # Manages dream sessions during idle periods for memory consolidation.
  # The scheduler detects inactivity and triggers multi-stage consolidation
  # including episodic clustering, pattern extraction, and synthesis.
  # =========================================================================
  dreaming:
    # Master switch for the dreaming system
    enabled: true

    # Idle detection
    idle_threshold_seconds: 300  # 5 minutes of inactivity before considering idle
    min_idle_duration: 60  # Must be idle for 1 minute before dreaming
    max_cpu_percent: 25.0  # Don't dream if CPU is busy

    # Scheduling (cron-like)
    # Formats: "minute hour day month dow"
    # Special values: "nightly", "hourly", "daily", "weekly", "never"
    schedules:
      - name: "nightly"
        cron_expression: "0 2 * * *"  # 2 AM daily
        enabled: true

    # Session configuration
    session:
      max_duration_seconds: 600  # 10 minutes max per dream session
      max_memories_to_process: 1000

      # Pipeline stages
      enable_episodic_clustering: true
      enable_pattern_extraction: true
      enable_recursive_synthesis: true
      enable_contradiction_resolution: true
      enable_semantic_promotion: true
      enable_dream_report: true

      # Stage-specific settings
      cluster_time_window_hours: 24.0  # Group memories within 24h
      pattern_min_frequency: 2  # Min occurrences to be a pattern
      synthesis_max_depth: 3  # Recursive synthesis depth
      auto_resolve_contradictions: false  # Require manual review
      promotion_ltp_threshold: 0.7  # LTP strength for WARM promotion

    # Reporting
    persist_reports: true
    report_path: "./data/dream_reports"
    report_include_memory_details: false  # Privacy: don't include full content in reports
